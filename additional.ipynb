{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozRY4e5cOd9y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dIsw0h1N1iA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hugblb66NjS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nPerforming hyperparameter tuning for Random Forest...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnPerforming hyperparameter tuning for Random Forest...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m param_grid_rf \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],  \u001b[38;5;66;03m# Example values, adjust as needed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m], \u001b[38;5;66;03m# Example values\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m], \u001b[38;5;66;03m# Example values\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]    \u001b[38;5;66;03m# Example values\u001b[39;00m\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 11\u001b[0m rf_for_tuning \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m grid_search_rf \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     14\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf_for_tuning,\n\u001b[0;32m     15\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_rf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting GridSearchCV for Random Forest (this may take a while)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# GridSearchCV for RandomForestRegressor\n",
    "print(\"\\\\nPerforming hyperparameter tuning for Random Forest...\")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],  # Example values, adjust as needed\n",
    "    'max_depth': [None, 10, 20], # Example values\n",
    "    'min_samples_split': [2, 5], # Example values\n",
    "    'min_samples_leaf': [1, 2]    # Example values\n",
    "}\n",
    "\n",
    "rf_for_tuning = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_for_tuning,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1, # Use all available cores\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for Random Forest (this may take a while)...\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "best_rf_model = grid_search_rf.best_estimator_ # This is your tuned Random Forest model\n",
    "\n",
    "print(f\"\\\\nBest hyperparameters for Random Forest: {best_rf_params}\")\n",
    "\n",
    "# Evaluate the tuned Random Forest model\n",
    "# You would add this model's results to your 'model_results' dictionary\n",
    "evaluate_model(best_rf_model, 'Random Forest (Tuned)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Update the overall best model if Random Forest performs better\n",
    "# For example, you might compare RMSE of best_xgb_model and best_rf_model\n",
    "# and assign the better one to a variable like 'final_best_model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ikuAaU-2NjKT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nPerforming hyperparameter tuning for Random Forest Regressor...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m      7\u001b[0m param_grid_rf \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m],  \u001b[38;5;66;03m# Reduced for quicker demonstration\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# Common choices for RandomForest\u001b[39;00m\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Ensure X_train and y_train are available from your data splitting cell\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# rf_model was already defined and trained once. We create a new instance for tuning.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m rf_for_tuning \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m grid_search_rf \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     20\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf_for_tuning,\n\u001b[0;32m     21\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_rf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting GridSearchCV for Random Forest Regressor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# Insert these snippets into Section 8 of your Jupyter Notebook,\n",
    "# after the XGBoost tuning section.\n",
    "\n",
    "# --- Hyperparameter Tuning for RandomForestRegressor ---\n",
    "print(\"\\\\nPerforming hyperparameter tuning for Random Forest Regressor...\")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 150],  # Reduced for quicker demonstration\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'] # Common choices for RandomForest\n",
    "}\n",
    "\n",
    "# Ensure X_train and y_train are available from your data splitting cell\n",
    "# rf_model was already defined and trained once. We create a new instance for tuning.\n",
    "rf_for_tuning = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_for_tuning,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=3, # 3-fold CV\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for Random Forest Regressor...\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "print(f\"\\\\nBest hyperparameters for Random Forest Regressor: {best_rf_params}\")\n",
    "\n",
    "# Evaluate the tuned Random Forest model\n",
    "evaluate_model(best_rf_model, 'Random Forest (Tuned)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning for GradientBoostingRegressor ---\n",
    "print(\"\\\\nPerforming hyperparameter tuning for Gradient Boosting Regressor...\")\n",
    "\n",
    "param_grid_gbr = {\n",
    "    'n_estimators': [100, 150], # Reduced\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# gbr_model was already defined. Create a new instance for tuning.\n",
    "gbr_for_tuning = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "grid_search_gbr = GridSearchCV(\n",
    "    estimator=gbr_for_tuning,\n",
    "    param_grid=param_grid_gbr,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for Gradient Boosting Regressor...\")\n",
    "grid_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "best_gbr_params = grid_search_gbr.best_params_\n",
    "best_gbr_model = grid_search_gbr.best_estimator_\n",
    "print(f\"\\\\nBest hyperparameters for Gradient Boosting Regressor: {best_gbr_params}\")\n",
    "\n",
    "evaluate_model(best_gbr_model, 'Gradient Boosting (Tuned)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning for DecisionTreeRegressor ---\n",
    "print(\"\\\\nPerforming hyperparameter tuning for Decision Tree Regressor...\")\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error'] # For newer scikit-learn versions\n",
    "    # For older scikit-learn, criterion might be 'mse', 'friedman_mse', 'mae'\n",
    "}\n",
    "\n",
    "# dt_model was already defined. Create a new instance for tuning.\n",
    "dt_for_tuning = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=dt_for_tuning,\n",
    "    param_grid=param_grid_dt,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for Decision Tree Regressor...\")\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "best_dt_params = grid_search_dt.best_params_\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "print(f\"\\\\nBest hyperparameters for Decision Tree Regressor: {best_dt_params}\")\n",
    "\n",
    "evaluate_model(best_dt_model, 'Decision Tree (Tuned)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning for Ridge Regression ---\n",
    "# For Ridge, often only 'alpha' is tuned. If using polynomial features,\n",
    "# the degree of polynomial can also be tuned, but this typically requires\n",
    "# a pipeline approach with GridSearchCV if not done manually.\n",
    "# The reference 'copy_of_used_cars_price_prediction.py' did a manual search.\n",
    "# Here's how you can use GridSearchCV for alpha with Ridge:\n",
    "\n",
    "print(\"\\\\nPerforming hyperparameter tuning for Ridge Regression...\")\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# ridge_model was already defined. Create a new instance for tuning.\n",
    "ridge_for_tuning = Ridge()\n",
    "\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=ridge_for_tuning,\n",
    "    param_grid=param_grid_ridge,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for Ridge Regression...\")\n",
    "# Note: Ridge doesn't have a random_state.\n",
    "# It's important that X_train and y_train are consistently preprocessed if you're comparing\n",
    "# this Ridge with other models that might have been trained on differently preprocessed data\n",
    "# (e.g. with polynomial features).\n",
    "# For simplicity, this assumes X_train, y_train are the standard preprocessed versions.\n",
    "grid_search_ridge.fit(X_train, y_train) # Use the same X_train as other models\n",
    "\n",
    "best_ridge_params = grid_search_ridge.best_params_\n",
    "best_ridge_model = grid_search_ridge.best_estimator_\n",
    "print(f\"\\\\nBest hyperparameters for Ridge Regression: {best_ridge_params}\")\n",
    "\n",
    "evaluate_model(best_ridge_model, 'Ridge Regression (Tuned)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning for SVR (Support Vector Regressor) ---\n",
    "# SVR can be computationally expensive to tune, especially with larger datasets.\n",
    "# The parameter grid and CV folds might need to be kept small.\n",
    "\n",
    "# print(\"\\\\nPerforming hyperparameter tuning for SVR...\")\n",
    "# print(\"WARNING: SVR tuning can be very slow.\")\n",
    "\n",
    "# param_grid_svr = {\n",
    "#     'C': [0.1, 1, 10], # Example values\n",
    "#     'kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'gamma': ['scale', 'auto'], # For 'rbf', 'poly', 'sigmoid'\n",
    "#     'degree': [2, 3] # For 'poly' kernel\n",
    "# }\n",
    "\n",
    "# svr_for_tuning = SVR()\n",
    "\n",
    "# grid_search_svr = GridSearchCV(\n",
    "#     estimator=svr_for_tuning,\n",
    "#     param_grid=param_grid_svr,\n",
    "#     cv=2,  # Smaller CV for speed\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1,\n",
    "#     scoring='neg_mean_squared_error'\n",
    "# )\n",
    "\n",
    "# print(\"Starting GridSearchCV for SVR (this might take a very long time)...\")\n",
    "# grid_search_svr.fit(X_train, y_train)\n",
    "\n",
    "# best_svr_params = grid_search_svr.best_params_\n",
    "# best_svr_model = grid_search_svr.best_estimator_\n",
    "# print(f\"\\\\nBest hyperparameters for SVR: {best_svr_params}\")\n",
    "\n",
    "# evaluate_model(best_svr_model, 'SVR (Tuned)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "# After running all tuning, you might want to select an overall best model\n",
    "# For example:\n",
    "# overall_best_model_name = None\n",
    "# lowest_rmse = float('inf')\n",
    "# for model_name, metrics in model_results.items():\n",
    "#     if \"(Tuned)\" in model_name or model_name in [\"Linear Regression\", \"Voting Regressor\"]: # Consider tuned models and simple ones\n",
    "#         if metrics['RMSE'] < lowest_rmse:\n",
    "#             lowest_rmse = metrics['RMSE']\n",
    "#             overall_best_model_name = model_name\n",
    "# print(f\"\\\\nOverall best performing model based on RMSE: {overall_best_model_name} with RMSE: {lowest_rmse:.2f}\")\n",
    "# Now you can decide which model object to save, e.g.\n",
    "# if overall_best_model_name == 'XGBoost (Tuned)':\n",
    "#     final_model_to_save = best_xgb_model\n",
    "# elif overall_best_model_name == 'Random Forest (Tuned)':\n",
    "#     final_model_to_save = best_rf_model\n",
    "# # ... and so on for other tuned models.\n",
    "# # Then use the pickle snippet to save 'final_model_to_save'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lstf4RU9Nwsa"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assume 'best_xgb_model' is your final, best-performing model\n",
    "# Or, if you tuned multiple and have a 'final_best_model' variable:\n",
    "# final_best_model = best_xgb_model # or best_rf_model, etc.\n",
    "\n",
    "model_to_save = best_xgb_model # Change this if you have a different final model variable\n",
    "\n",
    "# Define a filename for the saved model\n",
    "filename = 'final_sales_forecasting_model.pkl'\n",
    "\n",
    "# Save the model to disk\n",
    "try:\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model_to_save, file)\n",
    "    print(f\"\\\\nModel saved successfully as {filename}\")\n",
    "\n",
    "    # Example of how to load it later:\n",
    "    # with open(filename, 'rb') as file:\n",
    "    #     loaded_model = pickle.load(file)\n",
    "    # print(\"\\\\nModel loaded successfully.\")\n",
    "    # predictions = loaded_model.predict(X_test) # Example usage\n",
    "except Exception as e:\n",
    "    print(f\"\\\\nError saving model: {e}\")\n",
    "\n",
    "# If you also saved the LabelEncoders or Scalers, you should pickle them too,\n",
    "# as they are needed to preprocess new data the same way as training data.\n",
    "# For example, if you had a list of LabelEncoder objects:\n",
    "# label_encoders_to_save = {} # Assuming you stored them in a dict: {'col_name': encoder_object}\n",
    "# for col, le_obj in categorical_encoders.items(): # Assuming categorical_encoders is your dict of fitted encoders\n",
    "#    label_encoders_to_save[col] = le_obj\n",
    "\n",
    "# with open('label_encoders.pkl', 'wb') as file:\n",
    "#    pickle.dump(label_encoders_to_save, file)\n",
    "# print(\"Label encoders saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmvWoAueNjCA"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Determine your final best model after all tuning.\n",
    "# This is a placeholder logic. You'll need to implement how you select the 'final_best_model_object'.\n",
    "# For instance, you could iterate through model_results, find the model with the best score,\n",
    "# and then retrieve the corresponding Python object (e.g., best_xgb_model, best_rf_model).\n",
    "\n",
    "# Let's assume after comparison, 'best_rf_model' was chosen as the overall best.\n",
    "# (Replace this with your actual logic to select the best model object)\n",
    "final_best_model_object = None\n",
    "best_score = float('inf') # For RMSE, lower is better\n",
    "best_model_name_overall = ''\n",
    "\n",
    "# Example: find model with lowest RMSE among tuned ones\n",
    "for name, metrics in model_results.items():\n",
    "    if \"(Tuned)\" in name: # Focus on tuned models\n",
    "        if metrics['RMSE'] < best_score:\n",
    "            best_score = metrics['RMSE']\n",
    "            best_model_name_overall = name\n",
    "            # Map name back to model object\n",
    "            if name == 'XGBoost (Tuned)': final_best_model_object = best_xgb_model\n",
    "            elif name == 'Random Forest (Tuned)': final_best_model_object = best_rf_model\n",
    "            elif name == 'Gradient Boosting (Tuned)': final_best_model_object = best_gbr_model\n",
    "            elif name == 'Decision Tree (Tuned)': final_best_model_object = best_dt_model\n",
    "            elif name == 'Ridge Regression (Tuned)': final_best_model_object = best_ridge_model\n",
    "            # Add SVR if you run it:\n",
    "            # elif name == 'SVR (Tuned)': final_best_model_object = best_svr_model\n",
    "\n",
    "if final_best_model_object:\n",
    "    print(f\"\\\\nSelected '{best_model_name_overall}' as the final model to save with RMSE: {best_score:.4f}.\")\n",
    "    model_filename = 'final_sales_forecasting_model.pkl'\n",
    "    try:\n",
    "        with open(model_filename, 'wb') as file:\n",
    "            pickle.dump(final_best_model_object, file)\n",
    "        print(f\"Model saved successfully as {model_filename}\")\n",
    "\n",
    "        # Also remember to save any preprocessors (like LabelEncoders or the ColumnTransformer 'preprocessor')\n",
    "        # if they were fitted on the training data and are needed for new data.\n",
    "        with open('preprocessor_sales_forecasting.pkl', 'wb') as file:\n",
    "            pickle.dump(preprocessor, file) # Assuming 'preprocessor' is your fitted ColumnTransformer\n",
    "        print(\"Preprocessor saved successfully as preprocessor_sales_forecasting.pkl\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nError saving model or preprocessor: {e}\")\n",
    "else:\n",
    "    print(\"\\\\nNo final best model was identified to save. Please check tuning results.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBBpc77FZ6FNye7j6ODaMi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
